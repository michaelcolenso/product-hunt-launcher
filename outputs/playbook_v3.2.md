# The PH Top 5 Playbook v3.1

You've completed the Readiness Scorecard and identified your gaps. This playbook is your execution reference. Use it to close specific weaknesses before launch day and to run the 24-hour window with discipline. It is not a promise of placement and it is not a generic checklist you skim once. It is an operating manual for decisions: what to prepare, what to post, when to respond, and how to convert launch-day attention into retained users. Work in sequence, but prioritize the module tied to your lowest score category first.

<!-- EDITOR'S NOTE: Replaced the introduction to position the playbook as a reference companion anchored to Scorecard outputs. -->

## Jump to Your Gap

| Low Score In | Go To |
|---|---|
| Audience Infrastructure | Module 1, Phase 1 |
| Asset Quality | Module 2 + Appendix B |
| Network Activation | Module 1, Phase 3 |
| Timing & Positioning | Module 1, Phase 2 |
| Retention Infrastructure | Module 5 |

<!-- EDITOR'S NOTE: Added required navigation block so users can route directly from score gaps to execution modules. -->

## How To Use This Playbook

1. Start with the lowest score category from your Scorecard.
2. Select one module owner and one backup owner.
3. Time-box each checklist item with a completion date.
4. Treat launch day as operations, not inspiration.
5. Run a short post-launch review within 48 hours.

The right way to use this document is to convert it into calendar events and owned tasks. If a line item has no owner and no due date, treat it as incomplete.

---

## Module 1: The 30-Day Countdown Timeline

Do not launch next-day because you feel pressure. A 30-day runway lets you build intent, test assets, and reduce avoidable failures.

### Phase 1: Audience Infrastructure (Days 30 to 15)

Goal: Build measurable demand before launch, not on launch.

* [ ] Finalize the Core Loop: Ensure your onboarding is frictionless. Product Hunt traffic bounces quickly if users cannot complete first value in under 60 seconds.
* [ ] Create a "Launch Coming Soon" teaser on your website with one CTA tied to your waitlist.
* [ ] Build a Launch List: Collect emails specifically for launch updates and launch-only offers.
* [ ] Publish two proof-of-work updates in public channels (X, LinkedIn, Indie Hackers) that demonstrate product progress.
* [ ] Identify your first-hour responders: ask for explicit yes/no confirmation and track expected timezone.
* [ ] Log 10-20 meaningful comments on other Product Hunt launches in your niche.
* [ ] Run one launch-interest email and measure click-through intent.
* [ ] Create a simple amplification roster: partners, creators, and communities likely to share on launch day.

Execution notes:
- Your audience target should be concrete: for example, 150+ launch-intent contacts and 20+ confirmed first-hour supporters.
- Build in public with useful updates, not vague hype posts.
- Treat every pre-launch signal as data. If intent is weak, delay and repair.

In launches we've documented, teams that entered launch day with confirmed first-hour support and a tested waitlist message tended to hold momentum longer, though results varied significantly by product category and audience size.

<!-- EDITOR'S NOTE: Reframed broad launch performance claims into documented-pattern language with uncertainty. -->

### Phase 2: Timing & Positioning (Days 14 to 7)

Goal: Make your launch legible and relevant in context.

* [ ] Review the last 4-6 weeks of launches in your category and note traffic density patterns.
* [ ] Choose your Product Hunt category based on buyer intent, not where you think competition is easiest.
* [ ] Document your "why now" in one sentence and test it with 5-10 target users.
* [ ] Analyze at least two comparable launches: their tagline, thumbnail, first gallery image, and comment strategy.
* [ ] Decide your launch date and timing model with timezone and supporter availability in mind.
* [ ] Prepare one contingency play if the board is more competitive than expected.

Execution notes:
- Positioning should answer three questions in one pass: who this is for, what pain it removes, why this matters now.
- The goal is not novelty wording. The goal is immediate clarity.
- Category mismatch is a silent conversion killer.

### Phase 3: Network Activation (Days 6 to 1)

Goal: Design a coordinated, distributed engagement system.

* [ ] Finalize your Hunter plan: either confirmed hunter support or self-hunt with equivalent social proof prep.
* [ ] Prepare personalized outreach for priority contacts across email, DM, and community channels.
* [ ] Build a 24-hour engagement schedule with channel-by-channel responsibilities.
* [ ] Segment your contact list by relationship strength and expected response speed.
* [ ] Create a rapid-response answer bank for common launch questions.
* [ ] Assign comment response shifts so replies stay fast across the full day.
* [ ] Rehearse your first 90 minutes: maker comment, first social post, first DM wave.

Execution notes:
- A launch is won by sustained cadence, not one spike.
- Personalized asks outperform broadcast asks when attention is scarce.
- If nobody owns a timeslot, assume it is uncovered.

In launches we've documented, distributed schedules with clear owner handoffs performed more consistently than ad hoc posting, though outcomes varied by team size and existing audience strength.

<!-- EDITOR'S NOTE: Expanded Module 1 into score-aligned phases and replaced unsupported certainty language with defensible observation framing. -->

---

## Module 2: The Asset Optimization Checklist

Before you schedule, verify every public-facing asset against this checklist.

### 1. The Tagline (Most important copy)

* [ ] Is it under 60 characters?
* [ ] Does it state exactly what it does? (Bad: "Revolutionize your workflow". Better: "Automate freelance tax estimates in one click").
* [ ] Does it avoid buzzwords unless they are functionally necessary?
* [ ] Can a target user explain the value in under 5 seconds after reading it once?
* [ ] Does it describe an outcome, not just a feature?

Tagline workflow:
1. Draft 10 versions.
2. Test with 10 target users.
3. Remove ambiguous words.
4. Keep one line that survives comprehension testing.

### 2. The Thumbnail (The hook)

* [ ] Is it animated and under 3MB?
* [ ] Is it legible when scaled down to 40x40 pixels?
* [ ] Does it contrast clearly against a light background?
* [ ] Does the motion highlight one key concept instead of rotating through multiple messages?
* [ ] Have you tested it on both desktop and mobile?

In launches we've documented, clear high-contrast thumbnails often improved initial click interest, though the size of improvement varied significantly by category and competing launches that day.

### 3. The Gallery (The pitch)

* [ ] Image 1: Shows the product and value proposition without requiring text.
* [ ] Image 2: Shows the main workflow users complete first.
* [ ] Image 3: Shows evidence (usage metric, customer quote, or integration proof).
* [ ] Image 4: Shows offer terms or onboarding path, not just branding.
* [ ] Optional Image 5: Shows edge-case handling or advanced use case.

Gallery workflow:
1. Build three candidate versions of Image 1.
2. Run a no-context visual test with target users.
3. Keep the image that gets the fastest correct interpretation.

### 4. The Demo Video (Trust accelerator)

* [ ] Is the total runtime under 2 minutes?
* [ ] Does the core "aha" appear in the first 15 seconds?
* [ ] Is narration clear at 1.0x and 1.25x playback speed?
* [ ] Are captions available?
* [ ] Is the final CTA consistent with your page CTA?

Video workflow:
1. Start with result state, not a logo animation.
2. Show one complete success path.
3. End with one action request.

### 5. Page Copy and Maker Comment Readiness

* [ ] Is the description under 500 characters and outcome-first?
* [ ] Does your maker comment include the problem, solution, and one clear ask for feedback?
* [ ] Does your copy avoid over-claims and unsupported absolute statements?
* [ ] Have you prepared short answers for pricing, integrations, and roadmap questions?

Asset QA checkpoint:
- Run one final full-page review on desktop and mobile within 72 hours of launch.
- Fix all broken links and media before scheduling.

<!-- EDITOR'S NOTE: Preserved checklist structure while expanding each asset section into testable, implementation-level guidance. -->

---

## Module 3: Swipe Files (Customize Before Sending)

These are base structures, not scripts to send unchanged. Keep the structure, personalize each message, and remove anything that does not match your actual product.

### Swipe 1: Hunter Outreach Email

Send to 3-5 relevant hunters around Day 20.

Subject: Potential Hunt Fit - {Product Name} ({3-word descriptor})

Body:

Hi {Hunter First Name},

I have been following your recent hunts, especially {Specific recent hunt and one line about why it stood out}.

I am launching {Product Name} on {target week}. It helps {specific user segment} do {specific outcome} in {time/cost delta if known}.

I thought it might fit your audience because {one sentence showing fit from their past hunts}.

If you are open to it, I can share a 30-second demo and complete launch assets for review.

Demo link: {URL}

Thanks for considering it,
{Your Name}

Personalization checklist before send:
* [ ] Mention one real hunt they published recently.
* [ ] Keep your fit statement to one sentence.
* [ ] Remove generic flattery.
* [ ] Confirm your launch week is realistic before outreach.

### Swipe 2: Maker Comment Draft

Post immediately when the launch goes live.

Hi Product Hunt, I am {Your Name}, maker of {Product Name}.

Problem:
I kept running into {specific friction point} while trying to {job-to-be-done}. It was costing {time/money/error impact} every week.

What we built:
{Product Name} helps {user segment} {primary outcome} by:
- {Benefit 1 grounded in workflow}
- {Benefit 2 grounded in workflow}
- {Benefit 3 grounded in workflow}

Launch-day offer:
For the Product Hunt community, we are offering {offer details and expiry} with code {code}.

Feedback request:
If you try it today, I would value your input on {onboarding step} and {pricing/positioning question}.

I will be here throughout the day to answer every question.

Maker comment QA:
* [ ] Keep under 220 words.
* [ ] Ask for specific feedback, not generic support.
* [ ] Ensure the offer terms are valid in checkout.

### Swipe 3: Launch Day Email to Existing List

Send around 8:00 AM PST.

Subject: We just launched {Product Name} on Product Hunt

Body:

Hi {First Name},

Today is our Product Hunt launch day for {Product Name}.

If this product has helped you, I would appreciate your feedback on our launch page. Your comments help us understand what to improve next.

Launch page: {URL}

If you are evaluating it for the first time, we included a launch-day offer for the Product Hunt community: {offer details}.

Thanks for being early and honest with your feedback.

{Your Name}

Email QA:
* [ ] One link only.
* [ ] No language requesting votes directly.
* [ ] Clear expectation of what action to take.

### Swipe 4: Community Post (Slack or Discord)

Use only in communities where you are already active.

Message:

Sharing something I shipped today: {Product Name} is live on Product Hunt.

If you have 3 minutes, I would value feedback on {specific element}. We are trying to improve {specific outcome} for {user segment}.

Link: {URL}

If this is outside channel norms, happy to remove.

Community QA:
* [ ] Match tone to channel rules.
* [ ] Ask for feedback, not ranking support.
* [ ] Reply quickly to every follow-up.

<!-- EDITOR'S NOTE: Replaced template placeholders and unverifiable benchmark language with practical, customizable message frameworks. -->

---

## Module 4: Launch Day Hour-by-Hour Playbook

Operational principle: preserve response quality while maintaining cadence.

* 12:01 AM PST: Launch goes live. Post your maker comment immediately.
* 12:15 AM PST: Publish your first launch post on X or LinkedIn and tag relevant collaborators.
* 01:00 AM PST: Send first DM wave to inner-circle supporters asking for product feedback and comments.
* 03:00 AM PST: Verify link tracking, code redemption, and onboarding flow health.
* 06:00 AM PST: Post a short update with one user-relevant result or insight.
* 08:00 AM PST: Send your launch-day email to your existing audience.
* 10:00 AM PST: Post in relevant communities where you are an active contributor.
* 12:00 PM PST: Midday review. Reply to all comments with specific answers and follow-up questions.
* 02:00 PM PST: Trigger second outreach wave to contacts in later timezones.
* 04:00 PM PST: Publish a ranking update only if it adds context and a clear user value angle.
* 06:00 PM PST: Triage unresolved questions, support issues, and conversion blockers.
* 09:00 PM PST: Final community follow-up and comment cleanup.
* 11:59 PM PST: Capture end-of-day screenshots, metrics, and immediate post-mortem notes.

### Shift Management

* [ ] Assign a response owner for every 2-3 hour block.
* [ ] Keep one backup owner for overlap periods.
* [ ] Define escalation triggers (payment bug, onboarding outage, broken media).
* [ ] Track unanswered comments every 30 minutes.

### Real-Time Metrics to Monitor

* [ ] Product Hunt ranking movement by hour.
* [ ] Comment count and comment quality.
* [ ] Unique visitors and signup rate.
* [ ] Activation completion and payment conversion.
* [ ] Support ticket volume and first-response time.

## When Things Go Wrong - Launch Rescue Protocol

### Scenario A: It is noon. You are at #8. Momentum has stalled.

Diagnosis:
You likely exhausted your first-wave support and did not replace it with a second-wave audience. Midday stalls usually reflect poor channel sequencing, weak social proof updates, or delayed comment responses.

Priority actions:
1. Run a focused reactivation burst across your highest-trust channels. Send personalized messages to contacts who engaged with your pre-launch updates but have not shown up yet.
2. Refresh the narrative publicly. Post a concise update that highlights one concrete user outcome from the morning and asks for feedback on a specific product decision.
3. Tighten comment response turnaround to under 10 minutes for the next 90 minutes. Ask follow-up questions that pull deeper discussion.
4. Trigger one partner amplification slot you held in reserve for midday.

Execution target for next 3 hours:
Increase comment velocity and bring in fresh qualified visitors, not random traffic spikes. If ranking does not improve, your post-launch conversion work still benefits from higher quality discussion and better visitor intent.

### Scenario B: Upvotes are coming in but there are fewer than 5 comments.

Diagnosis:
You are getting passive support without conversation, which limits algorithmic visibility and reduces social proof quality for new visitors. This often happens when outreach asks were too broad and did not guide people toward meaningful feedback.

Priority actions:
1. Update all live outreach copy to request one specific response prompt, such as "What part of onboarding felt unclear?".
2. Seed five high-quality questions from trusted users who can ask about real use cases, pricing tradeoffs, and implementation detail.
3. Expand your maker comment with one short product insight that invites discussion, then respond quickly to every reply with specifics.
4. Pause low-context promotional posts and prioritize channels where thoughtful comments are more likely.

Execution target:
Move from passive reactions to active dialogue. Better comments improve credibility for new visitors and help you diagnose conversion issues while traffic is still live.

### Scenario C: A similar competitor launched the same day and is outranking you.

Diagnosis:
You are likely being compared side by side and losing on message clarity, timing concentration, or proof quality. Competing head-to-head requires sharper positioning, not louder posting.

Priority actions:
1. Rewrite your next public update around your distinct user segment and unique workflow outcome. Avoid naming the competitor directly.
2. Bring proof forward: highlight one validated usage metric, customer quote, or concrete integration advantage.
3. Redirect outreach to the audience segment where your fit is strongest instead of broadcasting broadly.
4. Prioritize comments that clarify differentiation. Every answer should connect to who gets the most value from your product.

Execution target:
Win the right comparison for the right audience. Even if ranking remains close, better-fit traffic leads to stronger activation and post-launch revenue outcomes.

<!-- EDITOR'S NOTE: Added required rescue protocol with diagnosis-first structure and priority-ordered responses for likely failure scenarios. -->

---

## Module 5: Post-Launch Revenue Operations (Retention Infrastructure)

Top-board placement is temporary. Revenue compounding is the operating target for Days 0-30.

### Operating Model

Use this module as an SOP, not a reference checklist.

- `Revenue Owner`: owns activation and paid conversion outcomes.
- `Lifecycle Owner`: owns email and in-app follow-through.
- `Support Owner`: owns blocker triage and resolution handoff.
- `Analyst Owner`: owns metric integrity and daily reporting.

Minimum operating coverage:
- Days 0-3: metric checks every 2 hours from 08:00 to 22:00 local time.
- Days 4-7: metric checks twice daily.
- Days 8-30: metric checks daily with weekly review.

### 24-Hour Post-Launch SOP (Day 0)

Objective: stabilize onboarding and capture qualified demand before intent decays.

1. `T+1 hour`: confirm analytics events for `signup`, `activation_milestone`, `trial_started`, and `paid` are flowing.
2. `T+2 hours`: segment all new users by acquisition channel and onboarding stage.
3. `T+4 hours`: send the first segmented follow-up based on onboarding status.
4. `T+6 hours`: review top support blockers and assign one owner per blocker with ETA.
5. `T+10 hours`: publish one transparent launch update with a concrete product lesson.
6. `T+20 hours`: update command center metrics and mark red/yellow/green status.

Day 0 SLA targets:
- Critical onboarding bug acknowledgment in 15 minutes.
- Critical onboarding fix or mitigation in 4 hours.
- Support first response under 20 minutes during staffed windows.

### 7-Day Conversion Sprint SOP (Days 1-7)

Objective: convert launch-day attention into activated behavior and first payments.

Day-by-day actions:
- Day 1: send welcome sequence with one explicit first-value task.
- Day 2: ship role-based use-case examples by segment.
- Day 3: run office hours and tag objections by theme.
- Day 4: publish a small product update based on top objections.
- Day 5: offer assisted onboarding to high-intent non-activated users.
- Day 6: review funnel drop-off by segment and create one targeted fix.
- Day 7: launch one retention or offer experiment with owner and success metric.

Daily metrics to log in `revenue_command_center.csv`:
- sessions
- signups
- activated_users
- paid_users
- activation_rate
- paid_conversion_rate
- revenue

Sprint thresholds:
- `Activation warning`: activation_rate below 20% for any high-volume channel.
- `Conversion warning`: paid_conversion_rate below 5% for activated cohort.
- `Revenue warning`: revenue flat or down for 48 hours with stable sessions.

### 30-Day Revenue Follow-Through SOP (Days 8-30)

Objective: convert short-term launch demand into repeatable channel and offer learning.

- Track launch cohort trial-to-paid separately from non-launch baseline.
- Review segment performance weekly (team size, role, channel, offer variant).
- Keep an active offer experiment backlog; close or iterate every test within 14 days.
- Document top three channels by paid conversion quality, not traffic volume.
- Publish one transparent recap with outcomes, misses, and next experiment.

### Revenue Rescue Protocols

#### Protocol A: High Traffic + Low Activation

Trigger:
- sessions are strong, but activation_rate stays below threshold for two consecutive checks.

Diagnosis:
- message and audience are pulling clicks, but onboarding path is not converting first intent into first value.

Owner sequence:
1. Revenue Owner opens incident ticket and sets target recovery window.
2. Support Owner tags top three onboarding blockers from live conversations.
3. Product or engineering owner ships a fast-path onboarding fix.
4. Lifecycle Owner sends a targeted "complete your first value step" message.

Recovery SLA:
- first mitigation in 2 hours
- first measurable improvement check within 6 hours

#### Protocol B: High Activation + Low Paid Conversion

Trigger:
- activation_rate is healthy, paid_conversion_rate remains below threshold for 48 hours.

Diagnosis:
- users reach initial value but do not perceive enough paid-plan value or pricing fit.

Owner sequence:
1. Revenue Owner reviews paywall and plan comparison friction.
2. Lifecycle Owner sends segment-specific paid-use-case proof.
3. Product marketing owner tests one revised offer or packaging message.
4. Analyst Owner compares conversion by segment and offer variant.

Recovery SLA:
- first offer or messaging test live within 24 hours
- paid conversion trend review every 12 hours until stable

#### Protocol C: Paid Conversion Drop After Day 3

Trigger:
- paid_conversion_rate declines for two daily periods after Day 3 while top-of-funnel input is stable.

Diagnosis:
- novelty demand has faded and lifecycle follow-through is not sustaining decision momentum.

Owner sequence:
1. Lifecycle Owner audits Day 3-7 sequence for relevance and clarity.
2. Support Owner compiles top objections from non-converted activated users.
3. Revenue Owner launches one reactivation burst for high-intent users.
4. Analyst Owner validates whether specific channels or segments are driving decline.

Recovery SLA:
- revised Day 3-7 sequence shipped within 24 hours
- reactivation campaign sent within 12 hours of sequence update

### Exit Criteria (Module 5 Complete)

Module 5 is considered operational when all conditions are true:
- command center is updated daily through Day 7 and weekly through Day 30
- at least one rescue protocol has an assigned owner and runbook ready
- launch cohort conversion is tracked separately from baseline
- one documented experiment is completed by Day 7 with clear decision outcome

In launches we documented, teams with explicit owners, SLAs, and recovery triggers reduced revenue leakage during Days 1-7, though effect size varied by onboarding complexity and pricing model.

---

## Appendix A: How to Research Your Hunter in 20 Minutes

Start by finding active hunters whose recent hunts overlap with your product category and buyer profile. Use Product Hunt search, the daily leaderboard, and category pages to identify candidates who publish consistently. Prioritize hunters with visible engagement quality, not just volume. Open each candidate's last 10 hunts and review three things: average comment depth, product relevance to your audience, and whether the maker-hunter collaboration appears active (for example, timely maker comments and thoughtful hunter framing).

Next, score fit by alignment rather than status. Ask: does this hunter repeatedly back products for your exact user type? Do their hunted products share your pricing model or onboarding complexity? If you are launching a workflow tool for technical teams, a hunter who mostly posts consumer apps is likely a weak match. Keep a simple fit rubric: category match, audience match, engagement quality, and consistency.

Then assess response likelihood. Signals that a hunter may reply include recent activity, direct interaction with makers, and clear public contact paths. Look for hunters who still comment on hunted products within days of launch; dormant profiles are low probability even if historically strong.

Finally, craft an outreach message based on their real work. Reference one recent hunt and explain exactly why your product fits that pattern. Keep it short, specific, and easy to evaluate in under 30 seconds. Include one demo link and one sentence describing the outcome your product delivers. Your objective is not persuasion theater. Your objective is a fast, credible fit check.

<!-- EDITOR'S NOTE: Replaced placeholder hunter database with a concrete methodology that users can execute immediately without fabricated data. -->

## Appendix B: 48-Hour Asset Hardening Checklist

Use this final pass to reduce avoidable launch-day errors.

* [ ] Re-test thumbnail rendering on desktop and mobile.
* [ ] Re-run tagline comprehension with at least 3 fresh testers.
* [ ] Validate all gallery images for resolution and readability.
* [ ] Verify demo video loads quickly and captions are visible.
* [ ] Confirm Product Hunt description length and clarity.
* [ ] Dry-run maker comment in plain text and check formatting.
* [ ] Test launch offer code in live checkout flow.
* [ ] Verify analytics events for signup, activation, and conversion.
* [ ] Confirm support contacts and escalation paths are posted internally.
* [ ] Lock launch-day schedule in calendar with owners.

### Final Readiness Trigger

You are ready to launch when:
- Scorecard result is at least 70.
- Lowest category has a specific mitigation plan.
- Every launch-day timeslot has a named owner.
- Signup-to-activation flow is tested and measurable.

If any of these remain incomplete, delay and repair. A one-week delay is cheaper than a poorly executed launch.

## Appendix C: Launch Decision Trees and Execution Scripts

This appendix is for the operator running the day in real time. Use it when conditions shift quickly and you need a simple decision path.

### Decision Tree 1: Ranking Stalls

Question 1: Did comment velocity decline in the last 60 minutes?
- If yes, run a comment-first recovery: reply to all open threads, ask one follow-up question in each, and send a targeted message to five trusted users who can add context-rich comments.
- If no, move to Question 2.

Question 2: Did visitor volume drop by more than 20% versus your previous 2-hour baseline?
- If yes, trigger distribution recovery: publish one concise update with a concrete user outcome and send a second-wave outreach batch segmented by timezone.
- If no, move to Question 3.

Question 3: Is signup conversion stable while ranking drops?
- If yes, prioritize conversion and retention tasks while maintaining launch cadence. You may still finish with strong business outcomes.
- If no, inspect message-to-page consistency and CTA clarity immediately.

### Decision Tree 2: Support Queue Spikes

Question 1: Are most tickets tied to one onboarding step?
- If yes, publish an in-app hint and update your maker comment with a short workaround.
- If no, move to Question 2.

Question 2: Are payment-related issues increasing?
- If yes, stop promotional pushes for 20 minutes and resolve payment path blockers first.
- If no, move to Question 3.

Question 3: Is first response time over your SLA?
- If yes, pull backup coverage and switch to short-form response templates.
- If no, continue normal operations.

### Decision Tree 3: High Traffic, Low Activation

Question 1: Is time-to-value above your pre-launch benchmark?
- If yes, shorten the first-run workflow and remove optional fields from onboarding.
- If no, move to Question 2.

Question 2: Are users dropping at account creation?
- If yes, review social sign-in reliability, password constraints, and verification email delay.
- If no, move to Question 3.

Question 3: Are users reaching the product but not triggering core action?
- If yes, add one guided prompt directing to the first success event.
- If no, inspect analytics event mapping for instrumentation gaps.

### Execution Script: First 90 Minutes

Use this script exactly once, then shift to live adaptation.

0-10 minutes:
* [ ] Publish maker comment.
* [ ] Verify launch page assets and links.
* [ ] Post first social update.

10-30 minutes:
* [ ] Send first priority DM wave.
* [ ] Reply to all comments in under 10 minutes.
* [ ] Confirm analytics events are firing.

30-60 minutes:
* [ ] Publish one follow-up context post if questions are repeating.
* [ ] Escalate any signup or checkout issue immediately.
* [ ] Capture top three objections from comments.

60-90 minutes:
* [ ] Send second outreach batch to a different segment.
* [ ] Update internal ops board: ranking, comments, conversion.
* [ ] Decide whether to run recovery actions or stay on planned cadence.

### Execution Script: Midday Stabilization Block

Run at 11:30 AM PST.

* [ ] Compare current ranking to target and baseline trend.
* [ ] Review unanswered comments and support queue age.
* [ ] Trigger one credibility post with concrete proof (metric, case snippet, or integration walkthrough).
* [ ] Send segmented outreach to later timezones.
* [ ] Re-validate launch offer and landing flow.
* [ ] Assign one person to conversion monitoring and one to comment depth.

### Execution Script: Last 4 Hours

Run at 8:00 PM PST.

* [ ] Prioritize thoughtful replies on high-visibility comments.
* [ ] Push one final high-context update to existing supporters.
* [ ] Avoid low-context promotional spam.
* [ ] Capture full metric snapshot every 60 minutes.
* [ ] Prepare post-close follow-up message and customer onboarding queue.

### Message Quality Guardrails

Use these guardrails across all channels:
- Ask for feedback on a specific part of the product.
- Avoid absolute promises and broad performance claims.
- Keep each message to one primary ask.
- Always match external claims to what users will see after click.
- If you cannot support a claim with evidence, remove it.

In launches we've documented, messages that requested concrete feedback generated better comment quality and more useful post-launch product insight, though the size of this effect varied by audience maturity.

### Operator Debrief Template (Run Within 24 Hours)

1. What channel produced the highest-quality visitors?
2. Which outreach segment had the fastest response time?
3. What objection appeared most frequently in comments?
4. Where did users drop in the onboarding flow?
5. Which asset created the most confusion?
6. Which operational decision had the highest positive impact?
7. What should change before the next launch?

Write answers while details are fresh. Your next launch quality is determined by the quality of this debrief, not by memory a month later.

<!-- EDITOR'S NOTE: Added operational decision trees and scripts to close the gap between checklist completion and in-day execution quality. -->
