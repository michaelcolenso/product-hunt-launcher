# The PH Top 5 Playbook v3.1

You've completed the Readiness Scorecard and identified your gaps. This playbook is your execution reference. Use it to close specific weaknesses before launch day and to run the 24-hour window with discipline. It is not a promise of placement and it is not a generic checklist you skim once. It is an operating manual for decisions: what to prepare, what to post, when to respond, and how to convert launch-day attention into retained users. Work in sequence, but prioritize the module tied to your lowest score category first.

<!-- EDITOR'S NOTE: Replaced the introduction to position the playbook as a reference companion anchored to Scorecard outputs. -->

## Jump to Your Gap

| Low Score In | Go To |
|---|---|
| Audience Infrastructure | Module 1, Phase 1 |
| Asset Quality | Module 2 + Appendix B |
| Network Activation | Module 1, Phase 3 |
| Timing & Positioning | Module 1, Phase 2 |
| Retention Infrastructure | Module 5 |

<!-- EDITOR'S NOTE: Added required navigation block so users can route directly from score gaps to execution modules. -->

## How To Use This Playbook

1. Start with the lowest score category from your Scorecard.
2. Select one module owner and one backup owner.
3. Time-box each checklist item with a completion date.
4. Treat launch day as operations, not inspiration.
5. Run a short post-launch review within 48 hours.

The right way to use this document is to convert it into calendar events and owned tasks. If a line item has no owner and no due date, treat it as incomplete.

---

## Module 1: The 30-Day Countdown Timeline

Do not launch next-day because you feel pressure. A 30-day runway lets you build intent, test assets, and reduce avoidable failures.

### Phase 1: Audience Infrastructure (Days 30 to 15)

Goal: Build measurable demand before launch, not on launch.

* [ ] Finalize the Core Loop: Ensure your onboarding is frictionless. Product Hunt traffic bounces quickly if users cannot complete first value in under 60 seconds.
* [ ] Create a "Launch Coming Soon" teaser on your website with one CTA tied to your waitlist.
* [ ] Build a Launch List: Collect emails specifically for launch updates and launch-only offers.
* [ ] Publish two proof-of-work updates in public channels (X, LinkedIn, Indie Hackers) that demonstrate product progress.
* [ ] Identify your first-hour responders: ask for explicit yes/no confirmation and track expected timezone.
* [ ] Log 10-20 meaningful comments on other Product Hunt launches in your niche.
* [ ] Run one launch-interest email and measure click-through intent.
* [ ] Create a simple amplification roster: partners, creators, and communities likely to share on launch day.

Execution notes:
- Your audience target should be concrete: for example, 150+ launch-intent contacts and 20+ confirmed first-hour supporters.
- Build in public with useful updates, not vague hype posts.
- Treat every pre-launch signal as data. If intent is weak, delay and repair.

In launches we've documented, teams that entered launch day with confirmed first-hour support and a tested waitlist message tended to hold momentum longer, though results varied significantly by product category and audience size.

<!-- EDITOR'S NOTE: Reframed broad launch performance claims into documented-pattern language with uncertainty. -->

### Phase 2: Timing & Positioning (Days 14 to 7)

Goal: Make your launch legible and relevant in context.

* [ ] Review the last 4-6 weeks of launches in your category and note traffic density patterns.
* [ ] Choose your Product Hunt category based on buyer intent, not where you think competition is easiest.
* [ ] Document your "why now" in one sentence and test it with 5-10 target users.
* [ ] Analyze at least two comparable launches: their tagline, thumbnail, first gallery image, and comment strategy.
* [ ] Decide your launch date and timing model with timezone and supporter availability in mind.
* [ ] Prepare one contingency play if the board is more competitive than expected.

Execution notes:
- Positioning should answer three questions in one pass: who this is for, what pain it removes, why this matters now.
- The goal is not novelty wording. The goal is immediate clarity.
- Category mismatch is a silent conversion killer.

### Phase 3: Network Activation (Days 6 to 1)

Goal: Design a coordinated, distributed engagement system.

* [ ] Finalize your Hunter plan: either confirmed hunter support or self-hunt with equivalent social proof prep.
* [ ] Prepare personalized outreach for priority contacts across email, DM, and community channels.
* [ ] Build a 24-hour engagement schedule with channel-by-channel responsibilities.
* [ ] Segment your contact list by relationship strength and expected response speed.
* [ ] Create a rapid-response answer bank for common launch questions.
* [ ] Assign comment response shifts so replies stay fast across the full day.
* [ ] Rehearse your first 90 minutes: maker comment, first social post, first DM wave.

Execution notes:
- A launch is won by sustained cadence, not one spike.
- Personalized asks outperform broadcast asks when attention is scarce.
- If nobody owns a timeslot, assume it is uncovered.

In launches we've documented, distributed schedules with clear owner handoffs performed more consistently than ad hoc posting, though outcomes varied by team size and existing audience strength.

<!-- EDITOR'S NOTE: Expanded Module 1 into score-aligned phases and replaced unsupported certainty language with defensible observation framing. -->

---

## Module 2: The Asset Optimization Checklist

Before you schedule, verify every public-facing asset against this checklist.

### 1. The Tagline (Most important copy)

* [ ] Is it under 60 characters?
* [ ] Does it state exactly what it does? (Bad: "Revolutionize your workflow". Better: "Automate freelance tax estimates in one click").
* [ ] Does it avoid buzzwords unless they are functionally necessary?
* [ ] Can a target user explain the value in under 5 seconds after reading it once?
* [ ] Does it describe an outcome, not just a feature?

Tagline workflow:
1. Draft 10 versions.
2. Test with 10 target users.
3. Remove ambiguous words.
4. Keep one line that survives comprehension testing.

### 2. The Thumbnail (The hook)

* [ ] Is it animated and under 3MB?
* [ ] Is it legible when scaled down to 40x40 pixels?
* [ ] Does it contrast clearly against a light background?
* [ ] Does the motion highlight one key concept instead of rotating through multiple messages?
* [ ] Have you tested it on both desktop and mobile?

In launches we've documented, clear high-contrast thumbnails often improved initial click interest, though the size of improvement varied significantly by category and competing launches that day.

### 3. The Gallery (The pitch)

* [ ] Image 1: Shows the product and value proposition without requiring text.
* [ ] Image 2: Shows the main workflow users complete first.
* [ ] Image 3: Shows evidence (usage metric, customer quote, or integration proof).
* [ ] Image 4: Shows offer terms or onboarding path, not just branding.
* [ ] Optional Image 5: Shows edge-case handling or advanced use case.

Gallery workflow:
1. Build three candidate versions of Image 1.
2. Run a no-context visual test with target users.
3. Keep the image that gets the fastest correct interpretation.

### 4. The Demo Video (Trust accelerator)

* [ ] Is the total runtime under 2 minutes?
* [ ] Does the core "aha" appear in the first 15 seconds?
* [ ] Is narration clear at 1.0x and 1.25x playback speed?
* [ ] Are captions available?
* [ ] Is the final CTA consistent with your page CTA?

Video workflow:
1. Start with result state, not a logo animation.
2. Show one complete success path.
3. End with one action request.

### 5. Page Copy and Maker Comment Readiness

* [ ] Is the description under 500 characters and outcome-first?
* [ ] Does your maker comment include the problem, solution, and one clear ask for feedback?
* [ ] Does your copy avoid over-claims and unsupported absolute statements?
* [ ] Have you prepared short answers for pricing, integrations, and roadmap questions?

Asset QA checkpoint:
- Run one final full-page review on desktop and mobile within 72 hours of launch.
- Fix all broken links and media before scheduling.

<!-- EDITOR'S NOTE: Preserved checklist structure while expanding each asset section into testable, implementation-level guidance. -->

---

## Module 3: Swipe Files (Customize Before Sending)

These are base structures, not scripts to send unchanged. Keep the structure, personalize each message, and remove anything that does not match your actual product.

### Swipe 1: Hunter Outreach Email

Send to 3-5 relevant hunters around Day 20.

Subject: Potential Hunt Fit - {Product Name} ({3-word descriptor})

Body:

Hi {Hunter First Name},

I have been following your recent hunts, especially {Specific recent hunt and one line about why it stood out}.

I am launching {Product Name} on {target week}. It helps {specific user segment} do {specific outcome} in {time/cost delta if known}.

I thought it might fit your audience because {one sentence showing fit from their past hunts}.

If you are open to it, I can share a 30-second demo and complete launch assets for review.

Demo link: {URL}

Thanks for considering it,
{Your Name}

Personalization checklist before send:
* [ ] Mention one real hunt they published recently.
* [ ] Keep your fit statement to one sentence.
* [ ] Remove generic flattery.
* [ ] Confirm your launch week is realistic before outreach.

### Swipe 2: Maker Comment Draft

Post immediately when the launch goes live.

Hi Product Hunt, I am {Your Name}, maker of {Product Name}.

Problem:
I kept running into {specific friction point} while trying to {job-to-be-done}. It was costing {time/money/error impact} every week.

What we built:
{Product Name} helps {user segment} {primary outcome} by:
- {Benefit 1 grounded in workflow}
- {Benefit 2 grounded in workflow}
- {Benefit 3 grounded in workflow}

Launch-day offer:
For the Product Hunt community, we are offering {offer details and expiry} with code {code}.

Feedback request:
If you try it today, I would value your input on {onboarding step} and {pricing/positioning question}.

I will be here throughout the day to answer every question.

Maker comment QA:
* [ ] Keep under 220 words.
* [ ] Ask for specific feedback, not generic support.
* [ ] Ensure the offer terms are valid in checkout.

### Swipe 3: Launch Day Email to Existing List

Send around 8:00 AM PST.

Subject: We just launched {Product Name} on Product Hunt

Body:

Hi {First Name},

Today is our Product Hunt launch day for {Product Name}.

If this product has helped you, I would appreciate your feedback on our launch page. Your comments help us understand what to improve next.

Launch page: {URL}

If you are evaluating it for the first time, we included a launch-day offer for the Product Hunt community: {offer details}.

Thanks for being early and honest with your feedback.

{Your Name}

Email QA:
* [ ] One link only.
* [ ] No language requesting votes directly.
* [ ] Clear expectation of what action to take.

### Swipe 4: Community Post (Slack or Discord)

Use only in communities where you are already active.

Message:

Sharing something I shipped today: {Product Name} is live on Product Hunt.

If you have 3 minutes, I would value feedback on {specific element}. We are trying to improve {specific outcome} for {user segment}.

Link: {URL}

If this is outside channel norms, happy to remove.

Community QA:
* [ ] Match tone to channel rules.
* [ ] Ask for feedback, not ranking support.
* [ ] Reply quickly to every follow-up.

<!-- EDITOR'S NOTE: Replaced template placeholders and unverifiable benchmark language with practical, customizable message frameworks. -->

---

## Module 4: Launch Day Hour-by-Hour Playbook

Operational principle: preserve response quality while maintaining cadence.

* 12:01 AM PST: Launch goes live. Post your maker comment immediately.
* 12:15 AM PST: Publish your first launch post on X or LinkedIn and tag relevant collaborators.
* 01:00 AM PST: Send first DM wave to inner-circle supporters asking for product feedback and comments.
* 03:00 AM PST: Verify link tracking, code redemption, and onboarding flow health.
* 06:00 AM PST: Post a short update with one user-relevant result or insight.
* 08:00 AM PST: Send your launch-day email to your existing audience.
* 10:00 AM PST: Post in relevant communities where you are an active contributor.
* 12:00 PM PST: Midday review. Reply to all comments with specific answers and follow-up questions.
* 02:00 PM PST: Trigger second outreach wave to contacts in later timezones.
* 04:00 PM PST: Publish a ranking update only if it adds context and a clear user value angle.
* 06:00 PM PST: Triage unresolved questions, support issues, and conversion blockers.
* 09:00 PM PST: Final community follow-up and comment cleanup.
* 11:59 PM PST: Capture end-of-day screenshots, metrics, and immediate post-mortem notes.

### Shift Management

* [ ] Assign a response owner for every 2-3 hour block.
* [ ] Keep one backup owner for overlap periods.
* [ ] Define escalation triggers (payment bug, onboarding outage, broken media).
* [ ] Track unanswered comments every 30 minutes.

### Real-Time Metrics to Monitor

* [ ] Product Hunt ranking movement by hour.
* [ ] Comment count and comment quality.
* [ ] Unique visitors and signup rate.
* [ ] Activation completion and payment conversion.
* [ ] Support ticket volume and first-response time.

## When Things Go Wrong - Launch Rescue Protocol

### Scenario A: It is noon. You are at #8. Momentum has stalled.

Diagnosis:
You likely exhausted your first-wave support and did not replace it with a second-wave audience. Midday stalls usually reflect poor channel sequencing, weak social proof updates, or delayed comment responses.

Priority actions:
1. Run a focused reactivation burst across your highest-trust channels. Send personalized messages to contacts who engaged with your pre-launch updates but have not shown up yet.
2. Refresh the narrative publicly. Post a concise update that highlights one concrete user outcome from the morning and asks for feedback on a specific product decision.
3. Tighten comment response turnaround to under 10 minutes for the next 90 minutes. Ask follow-up questions that pull deeper discussion.
4. Trigger one partner amplification slot you held in reserve for midday.

Execution target for next 3 hours:
Increase comment velocity and bring in fresh qualified visitors, not random traffic spikes. If ranking does not improve, your post-launch conversion work still benefits from higher quality discussion and better visitor intent.

### Scenario B: Upvotes are coming in but there are fewer than 5 comments.

Diagnosis:
You are getting passive support without conversation, which limits algorithmic visibility and reduces social proof quality for new visitors. This often happens when outreach asks were too broad and did not guide people toward meaningful feedback.

Priority actions:
1. Update all live outreach copy to request one specific response prompt, such as "What part of onboarding felt unclear?".
2. Seed five high-quality questions from trusted users who can ask about real use cases, pricing tradeoffs, and implementation detail.
3. Expand your maker comment with one short product insight that invites discussion, then respond quickly to every reply with specifics.
4. Pause low-context promotional posts and prioritize channels where thoughtful comments are more likely.

Execution target:
Move from passive reactions to active dialogue. Better comments improve credibility for new visitors and help you diagnose conversion issues while traffic is still live.

### Scenario C: A similar competitor launched the same day and is outranking you.

Diagnosis:
You are in a side-by-side comparison and losing the first-glance narrative. Visitors are deciding quickly on audience fit, workflow clarity, and visible proof. If your listing, maker comment, and outreach are not aligned around one clear use case, the competitor appears easier to understand. This is usually a positioning and evidence gap, not just a volume gap.

Priority actions:
1. Re-anchor all live messaging to one explicit user segment and one job-to-be-done. Repeat the same wording in your next post and every maker reply.
2. Publish proof before claims: one usage result, one customer quote, and one implementation detail that shows why your workflow is different.
3. Re-segment outreach to strongest-fit contacts. Ask for feedback on that exact use case instead of broad launch support.
4. In comments, answer with differentiation context: who benefits most, where your approach is stronger, and where it is not the best fit yet.

Execution target:
In the next 2-3 hours, improve engagement: more comments tied to your core use case, better-fit signups, and fewer generic reactions. You may not overtake immediately, but stronger fit should improve activation.

<!-- EDITOR'S NOTE: Expanded Scenario C to 150-200 words with diagnosis depth and priority-ordered differentiation actions. -->

---

## Module 5: Post-Launch Capitalization (Retention Infrastructure)

Top-board placement is short-lived. Business impact is decided in the next 7-30 days.

### 24-Hour Post-Launch Checklist

* [ ] Publish a thank-you follow-up with one concrete lesson from launch day.
* [ ] Send a segmented follow-up email to new signups based on onboarding stage.
* [ ] Resolve all critical onboarding and billing bugs discovered during launch.
* [ ] Export and tag every meaningful launch comment for follow-up.
* [ ] Confirm Product Hunt offer code behavior after launch-day traffic normalizes.

### 7-Day Conversion Sprint

* [ ] Day 1: Send welcome and activation sequence with one clear first-value milestone.
* [ ] Day 2: Send use-case examples based on role or segment.
* [ ] Day 3: Run support office hours and capture top objections.
* [ ] Day 4: Publish product update addressing top launch feedback.
* [ ] Day 5: Offer assisted onboarding to high-intent trial users.
* [ ] Day 6: Analyze activation funnel drop-off by segment.
* [ ] Day 7: Launch one retention experiment with owner and success metric.

### 30-Day Revenue Follow-Through

* [ ] Track trial-to-paid conversion for launch cohort separately.
* [ ] Compare cohort retention against non-launch baseline.
* [ ] Document the three highest ROI launch channels for future launches.
* [ ] Package a transparent launch recap for future social proof.

In launches we've documented, teams that responded quickly to onboarding friction and ran a seven-day conversion sprint saw stronger downstream retention, though results varied significantly by product complexity and price point.

<!-- EDITOR'S NOTE: Expanded post-launch section to emphasize retention operations over vanity outcomes. -->

---

## Appendix A: How to Research Your Hunter in 20 Minutes

Use a strict 20-minute workflow so hunter selection is based on evidence rather than brand recognition. Minute 0-5: build a candidate pool from Product Hunt category pages, the daily leaderboard, and search results for products adjacent to yours. Keep only hunters with visible recent activity. Ignore vanity signals at this stage; your objective is to find operators who are currently active and relevant.

Minute 5-12: evaluate fit by reviewing each candidate's last 10 hunts. For every hunt, log four quick checks: category overlap, target user overlap, launch complexity overlap, and engagement quality. Category overlap asks whether they repeatedly hunt products in your market. User overlap asks whether those products serve the same buyer profile you serve. Complexity overlap asks whether they typically back products with similar onboarding depth, pricing friction, and implementation effort. Engagement quality asks whether comments show real product discussion instead of one-line reactions.

Minute 12-16: score response likelihood. Favor hunters who have interacted with makers recently, leave thoughtful comments on hunt pages, and maintain a clear outreach path (email, X DMs, or linked contact form). De-prioritize dormant profiles, even if they were influential in the past. A smaller list of active hunters is more useful than a larger list of unreachable names.

Minute 16-20: draft one outreach message per top candidate using proof of fit from your review. Reference one specific recent hunt and state why your product matches their demonstrated interests. Keep the message easy to scan: one sentence on user segment, one sentence on outcome, one demo link, and one clear ask for fit feedback. Do not use template flattery or manufactured urgency. The method is simple: relevance evidence first, concise context second, and a low-friction next step. If you cannot explain fit in two sentences, keep researching before sending.

<!-- EDITOR'S NOTE: Expanded Appendix A into a 20-minute, step-timed hunter research methodology focused on fit and response likelihood. -->

## Appendix B: 48-Hour Asset Hardening Checklist

Use this final pass to reduce avoidable launch-day errors.

* [ ] Re-test thumbnail rendering on desktop and mobile.
* [ ] Re-run tagline comprehension with at least 3 fresh testers.
* [ ] Validate all gallery images for resolution and readability.
* [ ] Verify demo video loads quickly and captions are visible.
* [ ] Confirm Product Hunt description length and clarity.
* [ ] Dry-run maker comment in plain text and check formatting.
* [ ] Test launch offer code in live checkout flow.
* [ ] Verify analytics events for signup, activation, and conversion.
* [ ] Confirm support contacts and escalation paths are posted internally.
* [ ] Lock launch-day schedule in calendar with owners.

### Final Readiness Trigger

You are ready to launch when:
- Scorecard result is at least 70.
- Lowest category has a specific mitigation plan.
- Every launch-day timeslot has a named owner.
- Signup-to-activation flow is tested and measurable.

If any of these remain incomplete, delay and repair. A one-week delay is cheaper than a poorly executed launch.

## Appendix C: Launch Decision Trees and Execution Scripts

This appendix is for the operator running the day in real time. Use it when conditions shift quickly and you need a simple decision path.

### Decision Tree 1: Ranking Stalls

Question 1: Did comment velocity decline in the last 60 minutes?
- If yes, run a comment-first recovery: reply to all open threads, ask one follow-up question in each, and send a targeted message to five trusted users who can add context-rich comments.
- If no, move to Question 2.

Question 2: Is visitor volume showing a sustained, noticeable decline versus your previous 2-hour baseline across two checks?
- If yes, trigger distribution recovery: publish one concise update with a concrete user outcome and send a second-wave outreach batch segmented by timezone.
- If no, move to Question 3.

<!-- EDITOR'S NOTE: Reframed Decision Tree 1 Question 2 from a fixed percentage trigger to an uncertainty-aware sustained-decline check. -->

Question 3: Is signup conversion stable while ranking drops?
- If yes, prioritize conversion and retention tasks while maintaining launch cadence. You may still finish with strong business outcomes.
- If no, inspect message-to-page consistency and CTA clarity immediately.

### Decision Tree 2: Support Queue Spikes

Question 1: Are most tickets tied to one onboarding step?
- If yes, publish an in-app hint and update your maker comment with a short workaround.
- If no, move to Question 2.

Question 2: Are payment-related issues increasing?
- If yes, stop promotional pushes for 20 minutes and resolve payment path blockers first.
- If no, move to Question 3.

Question 3: Is first response time over your SLA?
- If yes, pull backup coverage and switch to short-form response templates.
- If no, continue normal operations.

### Decision Tree 3: High Traffic, Low Activation

Question 1: Is time-to-value above your pre-launch benchmark?
- If yes, shorten the first-run workflow and remove optional fields from onboarding.
- If no, move to Question 2.

Question 2: Are users dropping at account creation?
- If yes, review social sign-in reliability, password constraints, and verification email delay.
- If no, move to Question 3.

Question 3: Are users reaching the product but not triggering core action?
- If yes, add one guided prompt directing to the first success event.
- If no, inspect analytics event mapping for instrumentation gaps.

### Execution Script: First 90 Minutes

Use this script exactly once, then shift to live adaptation.

0-10 minutes:
* [ ] Publish maker comment.
* [ ] Verify launch page assets and links.
* [ ] Post first social update.

10-30 minutes:
* [ ] Send first priority DM wave.
* [ ] Reply to all comments in under 10 minutes.
* [ ] Confirm analytics events are firing.

30-60 minutes:
* [ ] Publish one follow-up context post if questions are repeating.
* [ ] Escalate any signup or checkout issue immediately.
* [ ] Capture top three objections from comments.

60-90 minutes:
* [ ] Send second outreach batch to a different segment.
* [ ] Update internal ops board: ranking, comments, conversion.
* [ ] Decide whether to run recovery actions or stay on planned cadence.

### Execution Script: Midday Stabilization Block

Run at 11:30 AM PST.

* [ ] Compare current ranking to target and baseline trend.
* [ ] Review unanswered comments and support queue age.
* [ ] Trigger one credibility post with concrete proof (metric, case snippet, or integration walkthrough).
* [ ] Send segmented outreach to later timezones.
* [ ] Re-validate launch offer and landing flow.
* [ ] Assign one person to conversion monitoring and one to comment depth.

### Execution Script: Last 4 Hours

Run at 8:00 PM PST.

* [ ] Prioritize thoughtful replies on high-visibility comments.
* [ ] Push one final high-context update to existing supporters.
* [ ] Avoid low-context promotional spam.
* [ ] Capture full metric snapshot every 60 minutes.
* [ ] Prepare post-close follow-up message and customer onboarding queue.

### Message Quality Guardrails

Use these guardrails across all channels:
- Ask for feedback on a specific part of the product.
- Avoid absolute promises and broad performance claims.
- Keep each message to one primary ask.
- Always match external claims to what users will see after click.
- If you cannot support a claim with evidence, remove it.

In launches we've documented, messages that requested concrete feedback generated better comment quality and more useful post-launch product insight, though the size of this effect varied by audience maturity.

### Operator Debrief Template (Run Within 24 Hours)

1. What channel produced the highest-quality visitors?
2. Which outreach segment had the fastest response time?
3. What objection appeared most frequently in comments?
4. Where did users drop in the onboarding flow?
5. Which asset created the most confusion?
6. Which operational decision had the highest positive impact?
7. What should change before the next launch?

Write answers while details are fresh. Your next launch quality is determined by the quality of this debrief, not by memory a month later.

<!-- EDITOR'S NOTE: Added operational decision trees and scripts to close the gap between checklist completion and in-day execution quality. -->
